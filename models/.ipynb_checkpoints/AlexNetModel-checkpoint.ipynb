{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*-coding\"utf-8-*-\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LRN(nn.Module):\n",
    "    def __init__(self, local_size=1, bias = 1.0, alpha=1.0, beta=5, ACROSS_CHANNELS=False):\n",
    "        super(LRN, self).__init__()\n",
    "        self.ACROSS_CHANNELS = ACROSS_CHANNELS # local_size为奇数，为偶数不确定怎么处理？\n",
    "        # True: 通道间做归一化(local_size表示求和通道个数) False：通道内做归一化(local_size表示求和区间边长)\n",
    "        if self.ACROSS_CHANNELS: # LRN处理方式一：在通道间进行归一化\n",
    "            self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1), # kernel_size 为 一个元组\n",
    "                    stride=1, # stride 可以为一个整数型(通道，H，W三者都一样)，或者是一个元组\n",
    "                    padding=(int((local_size-1.0)/2), 0, 0)) # 在通道上进行填充,在0.2.0_3版本上报错\n",
    "            # 经过average处理之后，不同通道的单元进行了求和，然后与kernel_size相除，后续需要乘以local_size\n",
    "        else: # LRN处理方式二：在通道内做归一化。在2D空间上进行单侧抑制处理\n",
    "            self.average=nn.AvgPool2d(kernel_size=local_size,\n",
    "                    stride=1,              \n",
    "                    padding=int((local_size-1.0)/2))\n",
    "        self.local_size = local_size\n",
    "        self.bias = bias\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.ACROSS_CHANNELS:\n",
    "            div = x.pow(2).unsqueeze(1) #　对不同通道的单元求平方，需要在dim=1处插入一个维度，变成ＮＣＤＨＷ(D是表示输入或输出通道)，关于nn.AvgPool3d请查看API手册\n",
    "            div = self.average(div).mul(self.local_size).squeeze(1) # 在通道间求和，平均，再乘以local_size变成求和的结果，在dim=1去掉\n",
    "            div = div.mul(self.alpha).add(self.bias).pow(self.beta) # 按照LRN公式进行处理\n",
    "        else:\n",
    "            div = x.pow(2)\n",
    "            div = self.average(div).mul(self.local_size*self.local_size) # 待确认？\n",
    "            div = div.mul(self.alpha).add(self.bias).pow(self.beta)\n",
    "        x = x.div(div)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes = 2): # 默认为两类，猫和狗\n",
    "#         super().__init__() # python3\n",
    "        super(AlexNet, self).__init__()\n",
    "        # 开始构建AlexNet网络模型，5层卷积，3层全连接层\n",
    "        # 5层卷积层\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            LRN(local_size=5, bias=1, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, groups=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            LRN(local_size=5, bias=1, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        # 3层全连接层\n",
    "        # 前向计算的时候，最开始输入需要进行view操作，将3D的tensor变为1D\n",
    "        self.fc6 = nn.Sequential(\n",
    "            nn.Linear(in_features=6*6*256, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.fc7 = nn.Sequential(\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.fc8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv5(self.conv4(self.conv3(self.conv2(self.conv1(x)))))\n",
    "        x = x.view(-1, 6*6*256)\n",
    "        x = self.fc8(self.fc7(self.fc6(x)))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = AlexNet(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
